==========

Model = llama2_7b | Dataset = longbench

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/longbench.json",prompt average 60.85, gen average 17.65
[BlockManager] shared_cache=False, debug=False
request_time=MetricData(p50=0.7592907823812695, p99=1.964888976386501, max=2.1628889763865)
prompt_time=MetricData(p50=0.036980472875870696, p99=0.048719780968423235, max=0.04897593576307199)
decode_time=MetricData(p50=0.07193164397849164, p99=0.7055515109090228, max=0.9029175193173136)
decode_max_time=MetricData(p50=0.5268557892393704, p99=1.0141414837844744, max=1.0335714418981723)
prompt_idle_time=MetricData(p50=0.3723977979603264, p99=0.593895471267008, max=0.5960294299461382)
decode_idle_time=MetricData(p50=0.778410290527111, p99=0.9756769717365096, max=0.9810054338594025)
Average prompt latency: 0.035739498316214785
Max prompt latency: 0.04897593576307199
Min prompt latency: 0.019629116552406213
Average generation latency: 0.12588563707179282
Max generation latency: 0.9029175193173136
Min generation latency: 0.014809959502491457
total time: 2.1828889763865
Thoughput: 45.81085024559404 r/s, 2787.590237444397 token/s
Total preemptions: 0
Prompt SLO Good Throughput: 100 r, 45.81085024559404 r/s
Max-Decode SLO Good Throughput: 13 r, 5.955410531927225 r/s
Prompt and Max-Decode SLO Good Throughput: 13 r, 5.955410531927225 r/s
Decode SLO Good Throughput: 74 r, 33.90002918173959 r/s
Prompt and Decode SLO Good Throughput: 74 r, 33.90002918173959 r/s
==========

Model = llama2_7b | Dataset = needle

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/needle.json",prompt average 1020.0, gen average 5.0
[BlockManager] shared_cache=False, debug=False
==========

Model = llama2_7b | Dataset = sharegpt

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/sharegpt.json",prompt average 99.8, gen average 433.2
[BlockManager] shared_cache=False, debug=False
==========

Model = llama2_7b | Dataset = bookcorpus

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/bookcorpus.json",prompt average 5.85, gen average 5.4
[BlockManager] shared_cache=False, debug=False
request_time=MetricData(p50=0.643396159444725, p99=1.7809155955255531, max=2.1628889763865)
prompt_time=MetricData(p50=0.03230283544013693, p99=0.048615963844334496, max=0.04897593576307199)
decode_time=MetricData(p50=0.08668937803556215, p99=0.8778333987103072, max=0.9757143814490601)
decode_max_time=MetricData(p50=0.47499699939433143, p99=1.0139554419392935, max=1.0335714418981723)
prompt_idle_time=MetricData(p50=0.3551447135012913, p99=0.590669336619121, max=0.5960294299461382)
decode_idle_time=MetricData(p50=0.8629692711005956, p99=1.0, max=1.0)
Average prompt latency: 0.029669590558564687
Max prompt latency: 0.03917014458964929
Min prompt latency: 0.019626082400888784
Average generation latency: 0.20622413507675233
Max generation latency: 0.9757143814490601
Min generation latency: 0.01402922474848974
total time: 1.4071602475861806
Thoughput: 71.06511157598315 r/s, 415.73090271950144 token/s
Total preemptions: 0
Prompt SLO Good Throughput: 200 r, 142.1302231519663 r/s
Max-Decode SLO Good Throughput: 39 r, 27.71539351463343 r/s
Prompt and Max-Decode SLO Good Throughput: 39 r, 27.71539351463343 r/s
Decode SLO Good Throughput: 134 r, 95.22724951181742 r/s
Prompt and Decode SLO Good Throughput: 134 r, 95.22724951181742 r/s
==========

Model = llama2_7b | Dataset = wikipedia

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/wikipedia.json",prompt average 3.9, gen average 87.3
[BlockManager] shared_cache=False, debug=False
request_time=MetricData(p50=0.7836946934871021, p99=10.00834307916115, max=10.606343079161151)
prompt_time=MetricData(p50=0.03103278399034179, p99=0.04835830011502899, max=0.04897593576307199)
decode_time=MetricData(p50=0.055767737650009856, p99=0.8775800237546811, max=0.9757143814490601)
decode_max_time=MetricData(p50=0.4850256369436518, p99=1.0139548324386582, max=1.0335714418981723)
prompt_idle_time=MetricData(p50=0.34389575273867684, p99=0.5884453643589529, max=0.5960294299461382)
decode_idle_time=MetricData(p50=0.7767522858948289, p99=1.0, max=1.0)
Average prompt latency: 0.02960534979888032
Max prompt latency: 0.03915665252810763
Min prompt latency: 0.019626013442899753
Average generation latency: 0.039977905164750714
Max generation latency: 0.15432901593382403
Min generation latency: 0.014671225423131576
total time: 10.686343079161151
Thoughput: 9.357738120443138 r/s, 36.49517866972823 token/s
Total preemptions: 0
Prompt SLO Good Throughput: 300 r, 28.073214361329413 r/s
Max-Decode SLO Good Throughput: 52 r, 4.8660238226304315 r/s
Prompt and Max-Decode SLO Good Throughput: 52 r, 4.8660238226304315 r/s
Decode SLO Good Throughput: 233 r, 21.80352982063251 r/s
Prompt and Decode SLO Good Throughput: 233 r, 21.80352982063251 r/s
==========

Model = llama2_7b | Dataset = opencompass

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/opencompass.json",prompt average 49.2, gen average 11.75
[BlockManager] shared_cache=False, debug=False
request_time=MetricData(p50=0.7500850652829663, p99=9.80834307916115, max=10.606343079161151)
prompt_time=MetricData(p50=0.030822642726323624, p99=0.048335741891528824, max=0.04897593576307199)
decode_time=MetricData(p50=0.05918891097487683, p99=0.780430297187823, max=0.9757143814490601)
decode_max_time=MetricData(p50=0.5033542241120499, p99=1.0139548324386582, max=1.0335714418981723)
prompt_idle_time=MetricData(p50=0.34229782831145505, p99=0.5851318387040582, max=0.5960294299461382)
decode_idle_time=MetricData(p50=0.7638531106530001, p99=1.0, max=1.0)
Average prompt latency: 0.030538894661344552
Max prompt latency: 0.04448639170848695
Min prompt latency: 0.019628351549715387
Average generation latency: 0.07827524139617403
Max generation latency: 0.4716076322027316
Min generation latency: 0.016072756406779656
total time: 1.338557551483081
Thoughput: 74.7072846357656 r/s, 3675.5984040796675 token/s
Total preemptions: 0
Prompt SLO Good Throughput: 400 r, 298.8291385430624 r/s
Max-Decode SLO Good Throughput: 64 r, 47.812662166889986 r/s
Prompt and Max-Decode SLO Good Throughput: 64 r, 47.812662166889986 r/s
Decode SLO Good Throughput: 326 r, 243.54574791259586 r/s
Prompt and Decode SLO Good Throughput: 326 r, 243.54574791259586 r/s
==========

Model = mistral_7b | Dataset = longbench

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/longbench.json",prompt average 60.85, gen average 17.65
[BlockManager] shared_cache=False, debug=False
request_time=MetricData(p50=0.7431354424449061, p99=3.4800647435014898, max=10.606343079161151)
prompt_time=MetricData(p50=0.03103278399034179, p99=0.04807938542124302, max=0.04897593576307199)
decode_time=MetricData(p50=0.06083317656715857, p99=0.780430297187823, max=0.9757143814490601)
decode_max_time=MetricData(p50=0.5041983104116738, p99=1.0139548324386582, max=1.0335714418981723)
prompt_idle_time=MetricData(p50=0.33561967081807154, p99=0.5831732888226179, max=0.5960294299461382)
decode_idle_time=MetricData(p50=0.7729678784709147, p99=1.0, max=1.0)
Average prompt latency: 0.03217607730067467
Max prompt latency: 0.047817870051235944
Min prompt latency: 0.018743616150476955
Average generation latency: 0.12158079615608008
Max generation latency: 0.8974271446772852
Min generation latency: 0.012837245794944696
total time: 2.0262564728418964
Thoughput: 49.35209404155361 r/s, 3003.0749224285373 token/s
Total preemptions: 0
Prompt SLO Good Throughput: 500 r, 246.7604702077681 r/s
Max-Decode SLO Good Throughput: 78 r, 38.49463335241182 r/s
Prompt and Max-Decode SLO Good Throughput: 78 r, 38.49463335241182 r/s
Decode SLO Good Throughput: 401 r, 197.90189710663 r/s
Prompt and Decode SLO Good Throughput: 401 r, 197.90189710663 r/s
==========

Model = mistral_7b | Dataset = needle

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/needle.json",prompt average 1020.0, gen average 5.0
[BlockManager] shared_cache=False, debug=False
==========

Model = mistral_7b | Dataset = sharegpt

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/sharegpt.json",prompt average 99.8, gen average 433.2
[BlockManager] shared_cache=False, debug=False
==========

Model = mistral_7b | Dataset = bookcorpus

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/bookcorpus.json",prompt average 5.85, gen average 5.4
[BlockManager] shared_cache=False, debug=False
request_time=MetricData(p50=0.7043790508418711, p99=3.2995250101750457, max=10.606343079161151)
prompt_time=MetricData(p50=0.030549363852357347, p99=0.047975568297154235, max=0.04897593576307199)
decode_time=MetricData(p50=0.06573039913272222, p99=0.874751573301883, max=0.9757143814490601)
decode_max_time=MetricData(p50=0.48478642041810305, p99=1.008209674020475, max=1.0335714418981723)
prompt_idle_time=MetricData(p50=0.3380680969653207, p99=0.5828275341572839, max=0.5960294299461382)
decode_idle_time=MetricData(p50=0.8077253777645006, p99=1.0, max=1.0)
Average prompt latency: 0.028499700466077454
Max prompt latency: 0.0374103604421856
Min prompt latency: 0.018740581998959525
Average generation latency: 0.2021667169700186
Max generation latency: 0.9684298587800217
Min generation latency: 0.012642062106444252
total time: 1.3644324526791496
Thoughput: 73.2905464126448 r/s, 428.74969651397214 token/s
Total preemptions: 0
Prompt SLO Good Throughput: 600 r, 439.7432784758688 r/s
Max-Decode SLO Good Throughput: 106 r, 77.6879791974035 r/s
Prompt and Max-Decode SLO Good Throughput: 106 r, 77.6879791974035 r/s
Decode SLO Good Throughput: 462 r, 338.602324426419 r/s
Prompt and Decode SLO Good Throughput: 462 r, 338.602324426419 r/s
==========

Model = mistral_7b | Dataset = wikipedia

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/wikipedia.json",prompt average 3.9, gen average 87.3
[BlockManager] shared_cache=False, debug=False
request_time=MetricData(p50=0.7503643242281667, p99=8.888029849040805, max=10.606343079161151)
prompt_time=MetricData(p50=0.030113308805287342, p99=0.04781943680847271, max=0.04897593576307199)
decode_time=MetricData(p50=0.056968875193028075, p99=0.8747230030952892, max=0.9757143814490601)
decode_max_time=MetricData(p50=0.4850256369436518, p99=1.0059341339600971, max=1.0335714418981723)
prompt_idle_time=MetricData(p50=0.33868100148297187, p99=0.5794424615323102, max=0.5960294299461382)
decode_idle_time=MetricData(p50=0.7762389838362269, p99=1.0, max=1.0)
Average prompt latency: 0.02842452345624967
Max prompt latency: 0.03740572948223467
Min prompt latency: 0.01874051304097049
Average generation latency: 0.03778731837237176
Max generation latency: 0.15188625521412688
Min generation latency: 0.012689791016896601
total time: 9.366029849040807
Thoughput: 10.676882479745801 r/s, 41.639841671008625 token/s
Total preemptions: 0
Prompt SLO Good Throughput: 700 r, 74.73817735822061 r/s
Max-Decode SLO Good Throughput: 121 r, 12.919027800492419 r/s
Prompt and Max-Decode SLO Good Throughput: 121 r, 12.919027800492419 r/s
Decode SLO Good Throughput: 561 r, 59.89731071137395 r/s
Prompt and Decode SLO Good Throughput: 561 r, 59.89731071137395 r/s
==========

Model = mistral_7b | Dataset = opencompass

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/opencompass.json",prompt average 49.2, gen average 11.75
[BlockManager] shared_cache=False, debug=False
request_time=MetricData(p50=0.7362011808199641, p99=8.688029849040806, max=10.606343079161151)
prompt_time=MetricData(p50=0.029914451424744568, p99=0.04771633781061201, max=0.04897593576307199)
decode_time=MetricData(p50=0.0571655985098011, p99=0.780401726981229, max=0.9757143814490601)
decode_max_time=MetricData(p50=0.4915697571295481, p99=1.0059341339600971, max=1.0335714418981723)
prompt_idle_time=MetricData(p50=0.33561967081807154, p99=0.5770941887510742, max=0.5960294299461382)
decode_idle_time=MetricData(p50=0.7768326335225169, p99=1.0, max=1.0)
Average prompt latency: 0.028839288047655655
Max prompt latency: 0.03934913264708462
Min prompt latency: 0.018742851147786125
Average generation latency: 0.07425056112355291
Max generation latency: 0.4660648236121182
Min generation latency: 0.013152945021016741
total time: 1.2803958185530737
Thoughput: 78.10084862117573 r/s, 3842.561752161846 token/s
Total preemptions: 0
Prompt SLO Good Throughput: 800 r, 624.8067889694058 r/s
Max-Decode SLO Good Throughput: 136 r, 106.217154124799 r/s
Prompt and Max-Decode SLO Good Throughput: 136 r, 106.217154124799 r/s
Decode SLO Good Throughput: 654 r, 510.7795499824893 r/s
Prompt and Decode SLO Good Throughput: 654 r, 510.7795499824893 r/s
==========

Model = internlm2_7b | Dataset = longbench

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/longbench.json",prompt average 60.85, gen average 17.65
[BlockManager] shared_cache=False, debug=False
request_time=MetricData(p50=0.733743255590496, p99=8.488029849040805, max=10.606343079161151)
prompt_time=MetricData(p50=0.030201718079138407, p99=0.04771633781061201, max=0.04897593576307199)
decode_time=MetricData(p50=0.05844084658071176, p99=0.780401726981229, max=0.9757143814490601)
decode_max_time=MetricData(p50=0.4935558464503409, p99=1.0059341339600971, max=1.0335714418981723)
prompt_idle_time=MetricData(p50=0.33377538196463585, p99=0.5770941887510742, max=0.5960294299461382)
decode_idle_time=MetricData(p50=0.7793592668003344, p99=1.0, max=1.0)
Average prompt latency: 0.03217607730067467
Max prompt latency: 0.047817870051235944
Min prompt latency: 0.018743616150476955
Average generation latency: 0.12158079615608008
Max generation latency: 0.8974271446772852
Min generation latency: 0.012837245794944696
total time: 2.0262564728418964
Thoughput: 49.35209404155361 r/s, 3003.0749224285373 token/s
Total preemptions: 0
Prompt SLO Good Throughput: 900 r, 444.16884637398255 r/s
Max-Decode SLO Good Throughput: 150 r, 74.02814106233042 r/s
Prompt and Max-Decode SLO Good Throughput: 150 r, 74.02814106233042 r/s
Decode SLO Good Throughput: 729 r, 359.77676556292585 r/s
Prompt and Decode SLO Good Throughput: 729 r, 359.77676556292585 r/s
==========

Model = internlm2_7b | Dataset = needle

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/needle.json",prompt average 1020.0, gen average 5.0
[BlockManager] shared_cache=False, debug=False
==========

Model = internlm2_7b | Dataset = sharegpt

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/sharegpt.json",prompt average 99.8, gen average 433.2
[BlockManager] shared_cache=False, debug=False
==========

Model = internlm2_7b | Dataset = bookcorpus

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/bookcorpus.json",prompt average 5.85, gen average 5.4
[BlockManager] shared_cache=False, debug=False
request_time=MetricData(p50=0.7098966527597856, p99=3.4668616112002986, max=10.606343079161151)
prompt_time=MetricData(p50=0.02992121876266477, p99=0.04769534634434862, max=0.04897593576307199)
decode_time=MetricData(p50=0.06169671656875656, p99=0.8747230030952892, max=0.9757143814490601)
decode_max_time=MetricData(p50=0.4845708035973241, p99=1.0059117339072028, max=1.0335714418981723)
prompt_idle_time=MetricData(p50=0.3354639401440671, p99=0.5738709981615652, max=0.5960294299461382)
decode_idle_time=MetricData(p50=0.8005775403522905, p99=1.0, max=1.0)
Average prompt latency: 0.028499700466077454
Max prompt latency: 0.0374103604421856
Min prompt latency: 0.018740581998959525
Average generation latency: 0.2021667169700186
Max generation latency: 0.9684298587800217
Min generation latency: 0.012642062106444252
total time: 1.3644324526791496
Thoughput: 73.2905464126448 r/s, 428.74969651397214 token/s
Total preemptions: 0
Prompt SLO Good Throughput: 1000 r, 732.905464126448 r/s
Max-Decode SLO Good Throughput: 178 r, 130.45717261450775 r/s
Prompt and Max-Decode SLO Good Throughput: 178 r, 130.45717261450775 r/s
Decode SLO Good Throughput: 790 r, 578.995316659894 r/s
Prompt and Decode SLO Good Throughput: 790 r, 578.995316659894 r/s
==========

Model = internlm2_7b | Dataset = wikipedia

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/wikipedia.json",prompt average 3.9, gen average 87.3
[BlockManager] shared_cache=False, debug=False
request_time=MetricData(p50=0.7410458819360105, p99=8.688029849040806, max=10.606343079161151)
prompt_time=MetricData(p50=0.02985303976504522, p99=0.04743898987406273, max=0.04897593576307199)
decode_time=MetricData(p50=0.05585232355518058, p99=0.8747230030952892, max=0.9757143814490601)
decode_max_time=MetricData(p50=0.48478642041810305, p99=1.0058803715510183, max=1.0335714418981723)
prompt_idle_time=MetricData(p50=0.3366153993267573, p99=0.5738386767907928, max=0.5960294299461382)
decode_idle_time=MetricData(p50=0.7803886887163294, p99=1.0, max=1.0)
Average prompt latency: 0.02842452345624967
Max prompt latency: 0.03740572948223467
Min prompt latency: 0.01874051304097049
Average generation latency: 0.03778731837237176
Max generation latency: 0.15188625521412688
Min generation latency: 0.012689791016896601
total time: 9.366029849040807
Thoughput: 10.676882479745801 r/s, 41.639841671008625 token/s
Total preemptions: 0
Prompt SLO Good Throughput: 1100 r, 117.44570727720381 r/s
Max-Decode SLO Good Throughput: 193 r, 20.606383185909397 r/s
Prompt and Max-Decode SLO Good Throughput: 193 r, 20.606383185909397 r/s
Decode SLO Good Throughput: 889 r, 94.91748524494017 r/s
Prompt and Decode SLO Good Throughput: 889 r, 94.91748524494017 r/s
==========

Model = internlm2_7b | Dataset = opencompass

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/opencompass.json",prompt average 49.2, gen average 11.75
[BlockManager] shared_cache=False, debug=False
request_time=MetricData(p50=0.73264568500781, p99=8.686029849040807, max=10.606343079161151)
prompt_time=MetricData(p50=0.029778892006432717, p99=0.047335172749973974, max=0.04897593576307199)
decode_time=MetricData(p50=0.056986726197810374, p99=0.780401726981229, max=0.9757143814490601)
decode_max_time=MetricData(p50=0.48830949688898484, p99=1.0058803715510183, max=1.0335714418981723)
prompt_idle_time=MetricData(p50=0.33377538196463585, p99=0.573603574993782, max=0.5960294299461382)
decode_idle_time=MetricData(p50=0.7788493878385926, p99=1.0, max=1.0)
Average prompt latency: 0.028839288047655655
Max prompt latency: 0.03934913264708462
Min prompt latency: 0.018742851147786125
Average generation latency: 0.07425056112355291
Max generation latency: 0.4660648236121182
Min generation latency: 0.013152945021016741
total time: 1.2803958185530737
Thoughput: 78.10084862117573 r/s, 3842.561752161846 token/s
Total preemptions: 0
Prompt SLO Good Throughput: 1200 r, 937.2101834541088 r/s
Max-Decode SLO Good Throughput: 208 r, 162.4497651320455 r/s
Prompt and Max-Decode SLO Good Throughput: 208 r, 162.4497651320455 r/s
Decode SLO Good Throughput: 982 r, 766.9503334599457 r/s
Prompt and Decode SLO Good Throughput: 982 r, 766.9503334599457 r/s
