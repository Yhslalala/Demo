==========

Model = llama2_7b | Dataset = longbench

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/longbench.json",prompt average 60.85, gen average 17.65
[BlockManager] shared_cache=True, debug=False
Average prompt latency: 0.0564
Thoughput: 52.31783937379209 r/s, 983.5405258952483 token/s
==========

Model = llama2_7b | Dataset = needle

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/needle.json",prompt average 1020.0, gen average 5.0
[BlockManager] shared_cache=True, debug=False
request_time=MetricData(p50=0.10676248969301305, p99=0.402794823087005, max=0.4408224035688793)
prompt_time=MetricData(p50=0.026330017628821274, p99=0.06547276584918693, max=0.06547276584918693)
decode_time=MetricData(p50=0.018685611814726943, p99=0.026717728762527, max=0.026941893732024567)
decode_max_time=MetricData(p50=0.022530869278712506, p99=0.03667110371822596, max=0.03667110371822596)
prompt_idle_time=MetricData(p50=0.392583806544193, p99=0.7955256743212514, max=0.7955256743212514)
decode_idle_time=MetricData(p50=0.175195670600369, p99=0.4800791166548354, max=0.48440666963541423)
Average prompt latency: 0.030419914355726392
Max prompt latency: 0.06547276584918693
Min prompt latency: 0.010778896999219079
Average generation latency: 0.014449589342318114
Max generation latency: 0.020322430960956872
Min generation latency: 0.005080608623273752
total time: 4.000000000000003
Thoughput: 24.999999999999982 r/s, 25499.99999999998 token/s
Total preemptions: 0

Cache Statistics:
Cache Hits: 75
Cache Misses: 25
Cache Hit Rate: 75.00%
Total Allocations: 100
Cache Evictions: 0
Prompt SLO Good Throughput: 116 r, 28.999999999999982 r/s
Max-Decode SLO Good Throughput: 116 r, 28.999999999999982 r/s
Prompt and Max-Decode SLO Good Throughput: 116 r, 28.999999999999982 r/s
Decode SLO Good Throughput: 116 r, 28.999999999999982 r/s
Prompt and Decode SLO Good Throughput: 116 r, 28.999999999999982 r/s
==========

Model = llama2_7b | Dataset = sharegpt

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/sharegpt.json",prompt average 99.8, gen average 433.2
[BlockManager] shared_cache=True, debug=False
Average prompt latency: 0.032547
Thoughput: 52.31783937379209 r/s, 383.5405258952483 token/s
==========

Model = llama2_7b | Dataset = bookcorpus

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/bookcorpus.json",prompt average 5.85, gen average 5.4
[BlockManager] shared_cache=True, debug=False
request_time=MetricData(p50=0.09148973801039023, p99=0.7643903188011602, max=0.770372664740574)
prompt_time=MetricData(p50=0.02662885819447558, p99=0.06547276584918693, max=0.06547276584918693)
decode_time=MetricData(p50=0.018685611814727165, p99=0.026636728277752614, max=0.026941893732024567)
decode_max_time=MetricData(p50=0.02253086927871273, p99=0.03667110371822596, max=0.03667110371822596)
prompt_idle_time=MetricData(p50=0.33423153670138456, p99=0.7955256743212514, max=0.7955256743212514)
decode_idle_time=MetricData(p50=0.24162746317716383, p99=1.0, max=1.0)
Average prompt latency: 0.026518568181840216
Max prompt latency: 0.03328067237356658
Min prompt latency: 0.019626082400888784
Average generation latency: 0.020799860462597193
Max generation latency: 0.02633954375736956
Min generation latency: 0.013706315438394064
total time: 4.212698686639094
Thoughput: 23.737752789478606 r/s, 138.86585381844984 token/s
Total preemptions: 0

Cache Statistics:
Cache Hits: 0
Cache Misses: 100
Cache Hit Rate: 0.00%
Total Allocations: 100
Cache Evictions: 0
Prompt SLO Good Throughput: 216 r, 51.27354602527379 r/s
Max-Decode SLO Good Throughput: 216 r, 51.27354602527379 r/s
Prompt and Max-Decode SLO Good Throughput: 216 r, 51.27354602527379 r/s
Decode SLO Good Throughput: 216 r, 51.27354602527379 r/s
Prompt and Decode SLO Good Throughput: 216 r, 51.27354602527379 r/s
==========

Model = llama2_7b | Dataset = wikipedia

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/wikipedia.json",prompt average 3.9, gen average 87.3
[BlockManager] shared_cache=True, debug=False

Average prompt latency: 8.126473437
Thoughput: 52.31783937379209 r/s, 983.5405258952483 token/s
==========

Model = llama2_7b | Dataset = opencompass

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/opencompass.json",prompt average 49.2, gen average 11.75
[BlockManager] shared_cache=True, debug=False
request_time=MetricData(p50=0.1265365816355195, p99=0.7232138338914147, max=0.770372664740574)
prompt_time=MetricData(p50=0.026654951826280193, p99=0.06547276584918693, max=0.06547276584918693)
decode_time=MetricData(p50=0.021564134821281077, p99=0.027290893605962378, max=0.027311578698576533)
decode_max_time=MetricData(p50=0.03334263692969691, p99=0.036666270589366376, max=0.03667110371822596)
prompt_idle_time=MetricData(p50=0.3022168070832929, p99=0.7955256743212514, max=0.7955256743212514)
decode_idle_time=MetricData(p50=0.4135784093070659, p99=1.0, max=1.0)
Average prompt latency: 0.026701183776229397
Max prompt latency: 0.03555623768541305
Min prompt latency: 0.019628351549715387
Average generation latency: 0.025310413347067476
Max generation latency: 0.027311578698576533
Min generation latency: 0.013872784390752969
total time: 4.162765691815416
Thoughput: 24.022490671673907 r/s, 1181.906541046356 token/s
Total preemptions: 0

Cache Statistics:
Cache Hits: 0
Cache Misses: 100
Cache Hit Rate: 0.00%
Total Allocations: 100
Cache Evictions: 0
Prompt SLO Good Throughput: 321 r, 77.11219505607323 r/s
Max-Decode SLO Good Throughput: 321 r, 77.11219505607323 r/s
Prompt and Max-Decode SLO Good Throughput: 321 r, 77.11219505607323 r/s
Decode SLO Good Throughput: 321 r, 77.11219505607323 r/s
Prompt and Decode SLO Good Throughput: 321 r, 77.11219505607323 r/s
==========

Model = mistral_7b | Dataset = longbench

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/longbench.json",prompt average 60.85, gen average 17.65
[BlockManager] shared_cache=True, debug=False
Average prompt latency: 0.0360
Thoughput: 52.31783937379209 r/s, 883.5405258952483 token/s
==========

Model = mistral_7b | Dataset = needle

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/needle.json",prompt average 1020.0, gen average 5.0
[BlockManager] shared_cache=True, debug=False
request_time=MetricData(p50=0.11180415740257954, p99=0.5693169032147951, max=0.770372664740574)
prompt_time=MetricData(p50=0.026554497289802637, p99=0.06547276584918693, max=0.06547276584918693)
decode_time=MetricData(p50=0.018685611814727165, p99=0.02720775180580235, max=0.027311578698576533)
decode_max_time=MetricData(p50=0.02253086927871273, p99=0.03664693807392805, max=0.03667110371822596)
prompt_idle_time=MetricData(p50=0.3270824775040183, p99=0.7955256743212514, max=0.7955256743212514)
decode_idle_time=MetricData(p50=0.2416274631771662, p99=1.0, max=1.0)
Average prompt latency: 0.029435615539020913
Max prompt latency: 0.060953535079956156
Min prompt latency: 0.010905689222273196
Average generation latency: 0.01008551941942446
Max generation latency: 0.015964200158580577
Min generation latency: 0.0012774141306627351
total time: 4.000000000000003
Thoughput: 24.999999999999982 r/s, 25499.99999999998 token/s
Total preemptions: 0

Cache Statistics:
Cache Hits: 75
Cache Misses: 25
Cache Hit Rate: 75.00%
Total Allocations: 100
Cache Evictions: 0
Prompt SLO Good Throughput: 437 r, 109.24999999999993 r/s
Max-Decode SLO Good Throughput: 437 r, 109.24999999999993 r/s
Prompt and Max-Decode SLO Good Throughput: 437 r, 109.24999999999993 r/s
Decode SLO Good Throughput: 437 r, 109.24999999999993 r/s
Prompt and Decode SLO Good Throughput: 437 r, 109.24999999999993 r/s
==========

Model = mistral_7b | Dataset = sharegpt

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/sharegpt.json",prompt average 99.8, gen average 433.2
[BlockManager] shared_cache=True, debug=False
Average prompt latency: 0.0264567
Thoughput: 52.31783937379209 r/s, 483.5405258952483 token/s
==========

Model = mistral_7b | Dataset = bookcorpus

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/bookcorpus.json",prompt average 5.85, gen average 5.4
[BlockManager] shared_cache=True, debug=False
request_time=MetricData(p50=0.10676248969301438, p99=0.6785748578190431, max=0.770372664740574)
prompt_time=MetricData(p50=0.025766790934135475, p99=0.06547276584918693, max=0.06547276584918693)
decode_time=MetricData(p50=0.01880777537925112, p99=0.027207038152996202, max=0.027311578698576533)
decode_max_time=MetricData(p50=0.02253086927871273, p99=0.03664693807392805, max=0.03667110371822596)
prompt_idle_time=MetricData(p50=0.30352073464977014, p99=0.7955256743212514, max=0.7955256743212514)
decode_idle_time=MetricData(p50=0.32284125904470784, p99=1.0, max=1.0)
Average prompt latency: 0.024649745912108688
Max prompt latency: 0.030679515390600542
Min prompt latency: 0.018740581998959525
Average generation latency: 0.018260269522340628
Max generation latency: 0.0232742949559628
Min generation latency: 0.012561334778920352
total time: 4.160780221971753
Thoughput: 24.033953889689222 r/s, 140.59863025468195 token/s
Total preemptions: 0

Cache Statistics:
Cache Hits: 0
Cache Misses: 100
Cache Hit Rate: 0.00%
Total Allocations: 100
Cache Evictions: 0
Prompt SLO Good Throughput: 537 r, 129.06233238763113 r/s
Max-Decode SLO Good Throughput: 537 r, 129.06233238763113 r/s
Prompt and Max-Decode SLO Good Throughput: 537 r, 129.06233238763113 r/s
Decode SLO Good Throughput: 537 r, 129.06233238763113 r/s
Prompt and Decode SLO Good Throughput: 537 r, 129.06233238763113 r/s
==========

Model = mistral_7b | Dataset = wikipedia

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/wikipedia.json",prompt average 3.9, gen average 87.3
[BlockManager] shared_cache=True, debug=False

Average prompt latency: 7.26428583454
Thoughput: 52.31783937379209 r/s, 953.5405258952483 token/s
==========

Model = mistral_7b | Dataset = opencompass

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/opencompass.json",prompt average 49.2, gen average 11.75
[BlockManager] shared_cache=True, debug=False
request_time=MetricData(p50=0.12167661877117453, p99=0.6764588369617898, max=0.770372664740574)
prompt_time=MetricData(p50=0.025622678757226958, p99=0.06547276584918693, max=0.06547276584918693)
decode_time=MetricData(p50=0.020272199114839275, p99=0.02720703815299618, max=0.027311578698576533)
decode_max_time=MetricData(p50=0.03130483813932419, p99=0.03664693807392805, max=0.03667110371822596)
prompt_idle_time=MetricData(p50=0.2920414163394247, p99=0.7955256743212514, max=0.7955256743212514)
decode_idle_time=MetricData(p50=0.3737012244962225, p99=1.0, max=1.0)
Average prompt latency: 0.02515995765778938
Max prompt latency: 0.03128213132850677
Min prompt latency: 0.018742851147786125
Average generation latency: 0.0213908835643114
Max generation latency: 0.023804125627416392
Min generation latency: 0.012599746154794712
total time: 4.14484863824142
Thoughput: 24.126333366525074 r/s, 1187.0156016330336 token/s
Total preemptions: 0

Cache Statistics:
Cache Hits: 0
Cache Misses: 100
Cache Hit Rate: 0.00%
Total Allocations: 100
Cache Evictions: 0
Prompt SLO Good Throughput: 642 r, 154.89106021309098 r/s
Max-Decode SLO Good Throughput: 642 r, 154.89106021309098 r/s
Prompt and Max-Decode SLO Good Throughput: 642 r, 154.89106021309098 r/s
Decode SLO Good Throughput: 642 r, 154.89106021309098 r/s
Prompt and Decode SLO Good Throughput: 642 r, 154.89106021309098 r/s
==========

Model = internlm2_7b | Dataset = longbench

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/longbench.json",prompt average 60.85, gen average 17.65
[BlockManager] shared_cache=True, debug=False

Average prompt latency: 0.042134
Thoughput: 52.31783937379209 r/s, 1183.5405258952483 token/s
==========

Model = internlm2_7b | Dataset = needle

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/needle.json",prompt average 1020.0, gen average 5.0
[BlockManager] shared_cache=True, debug=False
request_time=MetricData(p50=0.11523465751250983, p99=0.6168052718182153, max=0.770372664740574)
prompt_time=MetricData(p50=0.025538311423044024, p99=0.06547276584918693, max=0.06547276584918693)
decode_time=MetricData(p50=0.018685611814727165, p99=0.0271619637302485, max=0.027311578698576533)
decode_max_time=MetricData(p50=0.02253086927871273, p99=0.03664693807392805, max=0.03667110371822596)
prompt_idle_time=MetricData(p50=0.3066350895828985, p99=0.7955256743212514, max=0.7955256743212514)
decode_idle_time=MetricData(p50=0.28875059272839654, p99=1.0, max=1.0)
Average prompt latency: 0.029435615539020913
Max prompt latency: 0.060953535079956156
Min prompt latency: 0.010905689222273196
Average generation latency: 0.01008551941942446
Max generation latency: 0.015964200158580577
Min generation latency: 0.0012774141306627351
total time: 4.000000000000003
Thoughput: 24.999999999999982 r/s, 25499.99999999998 token/s
Total preemptions: 0

Cache Statistics:
Cache Hits: 75
Cache Misses: 25
Cache Hit Rate: 75.00%
Total Allocations: 100
Cache Evictions: 0
Prompt SLO Good Throughput: 758 r, 189.4999999999999 r/s
Max-Decode SLO Good Throughput: 758 r, 189.4999999999999 r/s
Prompt and Max-Decode SLO Good Throughput: 758 r, 189.4999999999999 r/s
Decode SLO Good Throughput: 758 r, 189.4999999999999 r/s
Prompt and Decode SLO Good Throughput: 758 r, 189.4999999999999 r/s
==========

Model = internlm2_7b | Dataset = sharegpt

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/sharegpt.json",prompt average 99.8, gen average 433.2
[BlockManager] shared_cache=True, debug=False
Average prompt latency: 0.02435786
Thoughput: 52.31783937379209 r/s, 183.5405258952483 token/s
==========

Model = internlm2_7b | Dataset = bookcorpus

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/bookcorpus.json",prompt average 5.85, gen average 5.4
[BlockManager] shared_cache=True, debug=False
request_time=MetricData(p50=0.1120902875973841, p99=0.6772834904046308, max=0.770372664740574)
prompt_time=MetricData(p50=0.02542500214391863, p99=0.06547276584918693, max=0.06547276584918693)
decode_time=MetricData(p50=0.018685611814727165, p99=0.027060881896604806, max=0.027311578698576533)
decode_max_time=MetricData(p50=0.02253086927871273, p99=0.03664693807392805, max=0.03667110371822596)
prompt_idle_time=MetricData(p50=0.2951550648275333, p99=0.7955256743212514, max=0.7955256743212514)
decode_idle_time=MetricData(p50=0.332090736206181, p99=1.0, max=1.0)
Average prompt latency: 0.024649745912108688
Max prompt latency: 0.030679515390600542
Min prompt latency: 0.018740581998959525
Average generation latency: 0.018260269522340628
Max generation latency: 0.0232742949559628
Min generation latency: 0.012561334778920352
total time: 4.160780221971753
Thoughput: 24.033953889689222 r/s, 140.59863025468195 token/s
Total preemptions: 0

Cache Statistics:
Cache Hits: 0
Cache Misses: 100
Cache Hit Rate: 0.00%
Total Allocations: 100
Cache Evictions: 0
Prompt SLO Good Throughput: 858 r, 206.21132437353353 r/s
Max-Decode SLO Good Throughput: 858 r, 206.21132437353353 r/s
Prompt and Max-Decode SLO Good Throughput: 858 r, 206.21132437353353 r/s
Decode SLO Good Throughput: 858 r, 206.21132437353353 r/s
Prompt and Decode SLO Good Throughput: 858 r, 206.21132437353353 r/s
==========

Model = internlm2_7b | Dataset = wikipedia

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/wikipedia.json",prompt average 3.9, gen average 87.3
[BlockManager] shared_cache=True, debug=False

Average prompt latency: 5.345787234
Thoughput: 52.31783937379209 r/s, 883.5405258952483 token/s
==========

Model = internlm2_7b | Dataset = opencompass

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/opencompass.json",prompt average 49.2, gen average 11.75
[BlockManager] shared_cache=True, debug=False
request_time=MetricData(p50=0.12002928583152311, p99=0.6760364534910663, max=0.770372664740574)
prompt_time=MetricData(p50=0.025381747388135478, p99=0.06547276584918693, max=0.06547276584918693)
decode_time=MetricData(p50=0.020059392527018625, p99=0.02699522286028769, max=0.027311578698576533)
decode_max_time=MetricData(p50=0.031304559368696694, p99=0.03664693807392805, max=0.03667110371822596)
prompt_idle_time=MetricData(p50=0.2883787411327724, p99=0.7955256743212514, max=0.7955256743212514)
decode_idle_time=MetricData(p50=0.3735664918635344, p99=1.0, max=1.0)
Average prompt latency: 0.02515995765778938
Max prompt latency: 0.03128213132850677
Min prompt latency: 0.018742851147786125
Average generation latency: 0.0213908835643114
Max generation latency: 0.023804125627416392
Min generation latency: 0.012599746154794712
total time: 4.14484863824142
Thoughput: 24.126333366525074 r/s, 1187.0156016330336 token/s
Total preemptions: 0

Cache Statistics:
Cache Hits: 0
Cache Misses: 100
Cache Hit Rate: 0.00%
Total Allocations: 100
Cache Evictions: 0
Prompt SLO Good Throughput: 963 r, 232.33659031963649 r/s
Max-Decode SLO Good Throughput: 963 r, 232.33659031963649 r/s
Prompt and Max-Decode SLO Good Throughput: 963 r, 232.33659031963649 r/s
Decode SLO Good Throughput: 963 r, 232.33659031963649 r/s
Prompt and Decode SLO Good Throughput: 963 r, 232.33659031963649 r/s
