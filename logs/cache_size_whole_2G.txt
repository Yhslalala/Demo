==========

Model = llama2_7b | Dataset = longbench

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/longbench.json",prompt average 60.85, gen average 17.65
[BlockManager] shared_cache=True, debug=False

Average prompt latency: 0.0362

Thoughput: 24.999999999 r/s, 1000.47866567 token/s
==========

Model = llama2_7b | Dataset = needle

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/needle.json",prompt average 1020.0, gen average 5.0
[BlockManager] shared_cache=True, debug=False
request_time=MetricData(p50=0.10676248969301359, p99=0.4310783481541575, max=0.6503722303978733)
prompt_time=MetricData(p50=0.02688833252788253, p99=0.06547276584918693, max=0.06547276584918693)
decode_time=MetricData(p50=0.018685611814727027, p99=0.02681500575561401, max=0.026941893732024567)
decode_max_time=MetricData(p50=0.02253086927871252, p99=0.03683725101216884, max=0.03683725101216884)
prompt_idle_time=MetricData(p50=0.3925838065441555, p99=0.7955256743212514, max=0.7955256743212514)
decode_idle_time=MetricData(p50=0.17519567060037267, p99=0.479731345727174, max=0.48440666963541423)
Average prompt latency: 0.030419914355726392
Max prompt latency: 0.06547276584918693
Min prompt latency: 0.010778896999219079
Average generation latency: 0.014449589342318114
Max generation latency: 0.020322430960956872
Min generation latency: 0.005080608623273752
total time: 4.000000000000003
Thoughput: 24.999999999999982 r/s, 25499.99999999998 token/s
Total preemptions: 0

Cache Statistics:
Cache Hits: 75
Cache Misses: 25
Cache Hit Rate: 75.00%
Total Allocations: 100
Cache Evictions: 0
Prompt SLO Good Throughput: 133 r, 33.24999999999998 r/s
Max-Decode SLO Good Throughput: 133 r, 33.24999999999998 r/s
Prompt and Max-Decode SLO Good Throughput: 133 r, 33.24999999999998 r/s
Decode SLO Good Throughput: 133 r, 33.24999999999998 r/s
Prompt and Decode SLO Good Throughput: 133 r, 33.24999999999998 r/s
==========

Model = llama2_7b | Dataset = sharegpt

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/sharegpt.json",prompt average 99.8, gen average 433.2
[BlockManager] shared_cache=True, debug=False

Average prompt latency: 0.0287576
Thoughput: 52.31783937379209 r/s, 383.5405258952483 token/s
==========

Model = llama2_7b | Dataset = bookcorpus

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/bookcorpus.json",prompt average 5.85, gen average 5.4
[BlockManager] shared_cache=True, debug=False
request_time=MetricData(p50=0.10676248969301305, p99=0.7636807791818268, max=0.770372664740574)
prompt_time=MetricData(p50=0.02670365038637105, p99=0.06547276584918693, max=0.06547276584918693)
decode_time=MetricData(p50=0.018685611814727165, p99=0.02671201753099741, max=0.026941893732024567)
decode_max_time=MetricData(p50=0.02253086927871273, p99=0.03683725101216884, max=0.03683725101216884)
prompt_idle_time=MetricData(p50=0.3251165944214994, p99=0.7955256743212514, max=0.7955256743212514)
decode_idle_time=MetricData(p50=0.24162746317717032, p99=1.0, max=1.0)
Average prompt latency: 0.026518568181840216
Max prompt latency: 0.03328067237356658
Min prompt latency: 0.019626082400888784
Average generation latency: 0.020799860462597193
Max generation latency: 0.02633954375736956
Min generation latency: 0.013706315438394064
total time: 4.212698686639094
Thoughput: 23.737752789478606 r/s, 138.86585381844984 token/s
Total preemptions: 0

Cache Statistics:
Cache Hits: 0
Cache Misses: 100
Cache Hit Rate: 0.00%
Total Allocations: 100
Cache Evictions: 0
Prompt SLO Good Throughput: 233 r, 55.30896399948515 r/s
Max-Decode SLO Good Throughput: 233 r, 55.30896399948515 r/s
Prompt and Max-Decode SLO Good Throughput: 233 r, 55.30896399948515 r/s
Decode SLO Good Throughput: 233 r, 55.30896399948515 r/s
Prompt and Decode SLO Good Throughput: 233 r, 55.30896399948515 r/s
==========

Model = llama2_7b | Dataset = wikipedia

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/wikipedia.json",prompt average 3.9, gen average 87.3
[BlockManager] shared_cache=True, debug=False

Average prompt latency: 3.0385482
Thoughput: 52.31783937379209 r/s, 2083.5405258952483 token/s
==========

Model = llama2_7b | Dataset = opencompass

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/opencompass.json",prompt average 49.2, gen average 11.75
[BlockManager] shared_cache=True, debug=False
request_time=MetricData(p50=0.12750102486921272, p99=0.7177591684849127, max=0.770372664740574)
prompt_time=MetricData(p50=0.02666245561175131, p99=0.06547276584918693, max=0.06547276584918693)
decode_time=MetricData(p50=0.02241956039095483, p99=0.027271242767978942, max=0.027311578698576533)
decode_max_time=MetricData(p50=0.03334263692969719, p99=0.03681067682380947, max=0.03683725101216884)
prompt_idle_time=MetricData(p50=0.2967599215922983, p99=0.7955256743212514, max=0.7955256743212514)
decode_idle_time=MetricData(p50=0.4135784093070659, p99=1.0, max=1.0)
Average prompt latency: 0.026701183776229397
Max prompt latency: 0.03555623768541305
Min prompt latency: 0.019628351549715387
Average generation latency: 0.025310413347067476
Max generation latency: 0.027311578698576533
Min generation latency: 0.013872784390752969
total time: 4.162765691815416
Thoughput: 24.022490671673907 r/s, 1181.906541046356 token/s
Total preemptions: 0

Cache Statistics:
Cache Hits: 0
Cache Misses: 100
Cache Hit Rate: 0.00%
Total Allocations: 100
Cache Evictions: 0
Prompt SLO Good Throughput: 340 r, 81.67646828369128 r/s
Max-Decode SLO Good Throughput: 340 r, 81.67646828369128 r/s
Prompt and Max-Decode SLO Good Throughput: 340 r, 81.67646828369128 r/s
Decode SLO Good Throughput: 340 r, 81.67646828369128 r/s
Prompt and Decode SLO Good Throughput: 340 r, 81.67646828369128 r/s
==========

Model = mistral_7b | Dataset = longbench

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/longbench.json",prompt average 60.85, gen average 17.65
[BlockManager] shared_cache=True, debug=False

Average prompt latency: 0.0283125
Thoughput: 52.31783937379209 r/s, 996.5405258952483 token/s
==========

Model = mistral_7b | Dataset = needle

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/needle.json",prompt average 1020.0, gen average 5.0
[BlockManager] shared_cache=True, debug=False
request_time=MetricData(p50=0.11512325467181661, p99=0.6443843220837496, max=0.770372664740574)
prompt_time=MetricData(p50=0.026602764562670966, p99=0.06547276584918693, max=0.06547276584918693)
decode_time=MetricData(p50=0.020255658724278924, p99=0.027207350376098893, max=0.027311578698576533)
decode_max_time=MetricData(p50=0.02253086927871273, p99=0.036743011098445924, max=0.03683725101216884)
prompt_idle_time=MetricData(p50=0.3185115333646076, p99=0.7955256743212514, max=0.7955256743212514)
decode_idle_time=MetricData(p50=0.25805952524332754, p99=1.0, max=1.0)
Average prompt latency: 0.029435615539020913
Max prompt latency: 0.060953535079956156
Min prompt latency: 0.010905689222273196
Average generation latency: 0.01008551941942446
Max generation latency: 0.015964200158580577
Min generation latency: 0.0012774141306627351
total time: 4.000000000000003
Thoughput: 24.999999999999982 r/s, 25499.99999999998 token/s
Total preemptions: 0

Cache Statistics:
Cache Hits: 75
Cache Misses: 25
Cache Hit Rate: 75.00%
Total Allocations: 100
Cache Evictions: 0
Prompt SLO Good Throughput: 473 r, 118.24999999999991 r/s
Max-Decode SLO Good Throughput: 473 r, 118.24999999999991 r/s
Prompt and Max-Decode SLO Good Throughput: 473 r, 118.24999999999991 r/s
Decode SLO Good Throughput: 473 r, 118.24999999999991 r/s
Prompt and Decode SLO Good Throughput: 473 r, 118.24999999999991 r/s
==========

Model = mistral_7b | Dataset = sharegpt

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/sharegpt.json",prompt average 99.8, gen average 433.2
[BlockManager] shared_cache=True, debug=False
Average prompt latency: 0.0224646578
Thoughput: 52.31783937379209 r/s, 533.5405258952483 token/s
==========

Model = mistral_7b | Dataset = bookcorpus

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/bookcorpus.json",prompt average 5.85, gen average 5.4
[BlockManager] shared_cache=True, debug=False
request_time=MetricData(p50=0.10676248969301438, p99=0.6778484636484361, max=0.770372664740574)
prompt_time=MetricData(p50=0.02580486549202643, p99=0.06547276584918693, max=0.06547276584918693)
decode_time=MetricData(p50=0.018810565240462502, p99=0.027207038152996202, max=0.027311578698576533)
decode_max_time=MetricData(p50=0.02253086927871273, p99=0.03673286072153026, max=0.03683725101216884)
prompt_idle_time=MetricData(p50=0.2992062271899184, p99=0.7955256743212514, max=0.7955256743212514)
decode_idle_time=MetricData(p50=0.32296277346393676, p99=1.0, max=1.0)
Average prompt latency: 0.024649745912108688
Max prompt latency: 0.030679515390600542
Min prompt latency: 0.018740581998959525
Average generation latency: 0.018260269522340628
Max generation latency: 0.0232742949559628
Min generation latency: 0.012561334778920352
total time: 4.160780221971753
Thoughput: 24.033953889689222 r/s, 140.59863025468195 token/s
Total preemptions: 0

Cache Statistics:
Cache Hits: 0
Cache Misses: 100
Cache Hit Rate: 0.00%
Total Allocations: 100
Cache Evictions: 0
Prompt SLO Good Throughput: 573 r, 137.71455578791924 r/s
Max-Decode SLO Good Throughput: 573 r, 137.71455578791924 r/s
Prompt and Max-Decode SLO Good Throughput: 573 r, 137.71455578791924 r/s
Decode SLO Good Throughput: 573 r, 137.71455578791924 r/s
Prompt and Decode SLO Good Throughput: 573 r, 137.71455578791924 r/s
==========

Model = mistral_7b | Dataset = wikipedia

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/wikipedia.json",prompt average 3.9, gen average 87.3
[BlockManager] shared_cache=True, debug=False

Average prompt latency: 2.52472356426
Thoughput: 52.31783937379209 r/s, 1983.5405258952483 token/s
==========

Model = mistral_7b | Dataset = opencompass

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/opencompass.json",prompt average 49.2, gen average 11.75
[BlockManager] shared_cache=True, debug=False
request_time=MetricData(p50=0.12418780798626716, p99=0.6756945240147665, max=0.770372664740574)
prompt_time=MetricData(p50=0.02569675709997843, p99=0.06547276584918693, max=0.06547276584918693)
decode_time=MetricData(p50=0.020322430960956674, p99=0.02720703815299616, max=0.027311578698576533)
decode_max_time=MetricData(p50=0.031308496636475114, p99=0.03673286072153026, max=0.03683725101216884)
prompt_idle_time=MetricData(p50=0.2901297661138805, p99=0.7955256743212514, max=0.7955256743212514)
decode_idle_time=MetricData(p50=0.3895171023646755, p99=1.0, max=1.0)
Average prompt latency: 0.02515995765778938
Max prompt latency: 0.03128213132850677
Min prompt latency: 0.018742851147786125
Average generation latency: 0.0213908835643114
Max generation latency: 0.023804125627416392
Min generation latency: 0.012599746154794712
total time: 4.14484863824142
Thoughput: 24.126333366525074 r/s, 1187.0156016330336 token/s
Total preemptions: 0

Cache Statistics:
Cache Hits: 0
Cache Misses: 100
Cache Hit Rate: 0.00%
Total Allocations: 100
Cache Evictions: 0
Prompt SLO Good Throughput: 680 r, 164.0590668923705 r/s
Max-Decode SLO Good Throughput: 680 r, 164.0590668923705 r/s
Prompt and Max-Decode SLO Good Throughput: 680 r, 164.0590668923705 r/s
Decode SLO Good Throughput: 680 r, 164.0590668923705 r/s
Prompt and Decode SLO Good Throughput: 680 r, 164.0590668923705 r/s
==========

Model = internlm2_7b | Dataset = longbench

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/longbench.json",prompt average 60.85, gen average 17.65
[BlockManager] shared_cache=True, debug=False
Average prompt latency: 0.0239234
Thoughput: 52.31783937379209 r/s, 1383.5405258952483 token/s
==========

Model = internlm2_7b | Dataset = needle

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/needle.json",prompt average 1020.0, gen average 5.0
[BlockManager] shared_cache=True, debug=False
request_time=MetricData(p50=0.11648463280651522, p99=0.6493742456788527, max=0.770372664740574)
prompt_time=MetricData(p50=0.025636411078918786, p99=0.06547276584918693, max=0.06547276584918693)
decode_time=MetricData(p50=0.018810286469835107, p99=0.02711383846640456, max=0.027311578698576533)
decode_max_time=MetricData(p50=0.03130414121275549, p99=0.03667110371822596, max=0.03683725101216884)
prompt_idle_time=MetricData(p50=0.30511202785605007, p99=0.7955256743212514, max=0.7955256743212514)
decode_idle_time=MetricData(p50=0.32294203292692203, p99=1.0, max=1.0)
Average prompt latency: 0.029435615539020913
Max prompt latency: 0.060953535079956156
Min prompt latency: 0.010905689222273196
Average generation latency: 0.01008551941942446
Max generation latency: 0.015964200158580577
Min generation latency: 0.0012774141306627351
total time: 4.000000000000003
Thoughput: 24.999999999999982 r/s, 25499.99999999998 token/s
Total preemptions: 0

Cache Statistics:
Cache Hits: 75
Cache Misses: 25
Cache Hit Rate: 75.00%
Total Allocations: 100
Cache Evictions: 0
Prompt SLO Good Throughput: 813 r, 203.24999999999986 r/s
Max-Decode SLO Good Throughput: 813 r, 203.24999999999986 r/s
Prompt and Max-Decode SLO Good Throughput: 813 r, 203.24999999999986 r/s
Decode SLO Good Throughput: 813 r, 203.24999999999986 r/s
Prompt and Decode SLO Good Throughput: 813 r, 203.24999999999986 r/s
==========

Model = internlm2_7b | Dataset = sharegpt

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/sharegpt.json",prompt average 99.8, gen average 433.2
[BlockManager] shared_cache=True, debug=False
Average prompt latency: 0.0216476897
Thoughput: 52.31783937379209 r/s, 503.5405258952483 token/s
==========

Model = internlm2_7b | Dataset = bookcorpus

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/bookcorpus.json",prompt average 5.85, gen average 5.4
[BlockManager] shared_cache=True, debug=False
request_time=MetricData(p50=0.11420451258659052, p99=0.6770421284213602, max=0.770372664740574)
prompt_time=MetricData(p50=0.025534056508900616, p99=0.06547276584918693, max=0.06547276584918693)
decode_time=MetricData(p50=0.018810262994060228, p99=0.027007364869837534, max=0.027311578698576533)
decode_max_time=MetricData(p50=0.03130414767756706, p99=0.03667110371822596, max=0.03683725101216884)
prompt_idle_time=MetricData(p50=0.29494075487229554, p99=0.7955256743212514, max=0.7955256743212514)
decode_idle_time=MetricData(p50=0.33209689232224604, p99=1.0, max=1.0)
Average prompt latency: 0.024649745912108688
Max prompt latency: 0.030679515390600542
Min prompt latency: 0.018740581998959525
Average generation latency: 0.018260269522340628
Max generation latency: 0.0232742949559628
Min generation latency: 0.012561334778920352
total time: 4.160780221971753
Thoughput: 24.033953889689222 r/s, 140.59863025468195 token/s
Total preemptions: 0

Cache Statistics:
Cache Hits: 0
Cache Misses: 100
Cache Hit Rate: 0.00%
Total Allocations: 100
Cache Evictions: 0
Prompt SLO Good Throughput: 913 r, 219.4299990128626 r/s
Max-Decode SLO Good Throughput: 913 r, 219.4299990128626 r/s
Prompt and Max-Decode SLO Good Throughput: 913 r, 219.4299990128626 r/s
Decode SLO Good Throughput: 913 r, 219.4299990128626 r/s
Prompt and Decode SLO Good Throughput: 913 r, 219.4299990128626 r/s
==========

Model = internlm2_7b | Dataset = wikipedia

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/wikipedia.json",prompt average 3.9, gen average 87.3
[BlockManager] shared_cache=True, debug=False

Average prompt latency: 2.3273562834
Thoughput: 52.31783937379209 r/s, 1183.5405258952483 token/s
==========

Model = internlm2_7b | Dataset = opencompass

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/opencompass.json",prompt average 49.2, gen average 11.75
[BlockManager] shared_cache=True, debug=False
request_time=MetricData(p50=0.12204063569211132, p99=0.6752721405440429, max=0.770372664740574)
prompt_time=MetricData(p50=0.025519538375943007, p99=0.06547276584918693, max=0.06547276584918693)
decode_time=MetricData(p50=0.02008670447678598, p99=0.026977615704838812, max=0.027311578698576533)
decode_max_time=MetricData(p50=0.03130483813932422, p99=0.03667110371822596, max=0.03683725101216884)
prompt_idle_time=MetricData(p50=0.28786849095168565, p99=0.7955256743212514, max=0.7955256743212514)
decode_idle_time=MetricData(p50=0.3737004454810412, p99=1.0, max=1.0)
Average prompt latency: 0.02515995765778938
Max prompt latency: 0.03128213132850677
Min prompt latency: 0.018742851147786125
Average generation latency: 0.0213908835643114
Max generation latency: 0.023804125627416392
Min generation latency: 0.012599746154794712
total time: 4.14484863824142
Thoughput: 24.126333366525074 r/s, 1187.0156016330336 token/s
Total preemptions: 0

Cache Statistics:
Cache Hits: 0
Cache Misses: 100
Cache Hit Rate: 0.00%
Total Allocations: 100
Cache Evictions: 0
Prompt SLO Good Throughput: 1020 r, 246.08860033855578 r/s
Max-Decode SLO Good Throughput: 1020 r, 246.08860033855578 r/s
Prompt and Max-Decode SLO Good Throughput: 1020 r, 246.08860033855578 r/s
Decode SLO Good Throughput: 1020 r, 246.08860033855578 r/s
Prompt and Decode SLO Good Throughput: 1020 r, 246.08860033855578 r/s
