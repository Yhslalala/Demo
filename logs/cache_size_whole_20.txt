==========

Model = llama2_7b | Dataset = longbench

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/longbench.json",prompt average 60.85, gen average 17.65
[BlockManager] shared_cache=True, debug=False
request_time=MetricData(p50=0.30153761679753754, p99=1.6724740226428976, max=1.8704740226428964)
prompt_time=MetricData(p50=0.023087293692333838, p99=0.048719780968423235, max=0.04897593576307199)
decode_time=MetricData(p50=0.03532063261880761, p99=0.09428499477200672, max=0.10789018187944072)
decode_max_time=MetricData(p50=0.061347776778529206, p99=0.21911414634660148, max=0.23854410446029958)
prompt_idle_time=MetricData(p50=0.4185413034664544, p99=0.8995860213888452, max=0.9207213061720936)
decode_idle_time=MetricData(p50=0.5794950391539704, p99=0.8451138552940637, max=0.8642297090126606)
Average prompt latency: 0.022639840437420605
Max prompt latency: 0.04897593576307199
Min prompt latency: 0.0020984607662047905
Average generation latency: 0.03477197936979559
Max generation latency: 0.10789018187944072
Min generation latency: 0.013841917697114993
total time: 1.9113939183446793
Thoughput: 52.31783937379209 r/s, 3183.5405258952483 token/s
Total preemptions: 0

Cache Statistics:
Cache Hits: 51
Cache Misses: 49
Cache Hit Rate: 51.00%
Total Allocations: 100
Cache Evictions: 0
Prompt SLO Good Throughput: 100 r, 52.31783937379209 r/s
Max-Decode SLO Good Throughput: 91 r, 47.6092338301508 r/s
Prompt and Max-Decode SLO Good Throughput: 91 r, 47.6092338301508 r/s
Decode SLO Good Throughput: 100 r, 52.31783937379209 r/s
Prompt and Decode SLO Good Throughput: 100 r, 52.31783937379209 r/s
==========

Model = llama2_7b | Dataset = needle

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/needle.json",prompt average 1020.0, gen average 5.0
[BlockManager] shared_cache=True, debug=False
request_time=MetricData(p50=0.17593199531724896, p99=1.4849101039778336, max=1.8704740226428964)
prompt_time=MetricData(p50=0.01481261025795566, p99=0.07035642535869882, max=0.09025642535869892)
decode_time=MetricData(p50=0.027801850861334937, p99=0.08154041878594187, max=0.10789018187944072)
decode_max_time=MetricData(p50=0.06113145077165072, p99=0.18975064603322028, max=0.23854410446029958)
prompt_idle_time=MetricData(p50=0.2910601640921348, p99=0.8933685283228275, max=0.9207213061720936)
decode_idle_time=MetricData(p50=0.5582332334785467, p99=0.8159116668337673, max=0.8642297090126606)
Average prompt latency: 0.017710352452489305
Max prompt latency: 0.09025642535869892
Min prompt latency: 0.0065492078968202305
Average generation latency: 0.02256386032559386
Max generation latency: 0.08141307383792515
Min generation latency: 0.0014557541549223563
total time: 1.0031532458370114
Thoughput: 99.68566658682539 r/s, 101679.3799185619 token/s
Total preemptions: 0

Cache Statistics:
Cache Hits: 99
Cache Misses: 1
Cache Hit Rate: 99.00%
Total Allocations: 100
Cache Evictions: 0
Prompt SLO Good Throughput: 200 r, 199.37133317365078 r/s
Max-Decode SLO Good Throughput: 191 r, 190.3996231808365 r/s
Prompt and Max-Decode SLO Good Throughput: 191 r, 190.3996231808365 r/s
Decode SLO Good Throughput: 200 r, 199.37133317365078 r/s
Prompt and Decode SLO Good Throughput: 200 r, 199.37133317365078 r/s
==========

Model = llama2_7b | Dataset = sharegpt

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/sharegpt.json",prompt average 99.8, gen average 433.2
[BlockManager] shared_cache=True, debug=False
==========

Model = llama2_7b | Dataset = bookcorpus

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/bookcorpus.json",prompt average 5.85, gen average 5.4
[BlockManager] shared_cache=True, debug=False
request_time=MetricData(p50=0.1604577131666521, p99=1.4789569544442358, max=1.8704740226428964)
prompt_time=MetricData(p50=0.020752032289724748, p99=0.0655206024442819, max=0.09025642535869892)
decode_time=MetricData(p50=0.030976183995040818, p99=0.09414837017717838, max=0.10789018187944072)
decode_max_time=MetricData(p50=0.061347776778529206, p99=0.18945602746645074, max=0.23854410446029958)
prompt_idle_time=MetricData(p50=0.33233754501639434, p99=0.8985811071685874, max=0.9207213061720936)
decode_idle_time=MetricData(p50=0.5960495704310155, p99=1.0, max=1.0)
Average prompt latency: 0.02195602504058577
Max prompt latency: 0.038878958331928254
Min prompt latency: 0.002908921431264755
Average generation latency: 0.04038861167956382
Max generation latency: 0.10404627960263038
Min generation latency: 0.009815487764644162
total time: 1.188406170827457
Thoughput: 84.14631500135391 r/s, 492.2559427579204 token/s
Total preemptions: 0

Cache Statistics:
Cache Hits: 51
Cache Misses: 49
Cache Hit Rate: 51.00%
Total Allocations: 100
Cache Evictions: 0
Prompt SLO Good Throughput: 300 r, 252.43894500406174 r/s
Max-Decode SLO Good Throughput: 291 r, 244.8657766539399 r/s
Prompt and Max-Decode SLO Good Throughput: 291 r, 244.8657766539399 r/s
Decode SLO Good Throughput: 300 r, 252.43894500406174 r/s
Prompt and Decode SLO Good Throughput: 300 r, 252.43894500406174 r/s
==========

Model = llama2_7b | Dataset = wikipedia

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/wikipedia.json",prompt average 3.9, gen average 87.3
[BlockManager] shared_cache=True, debug=False
==========

Model = llama2_7b | Dataset = opencompass

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/opencompass.json",prompt average 49.2, gen average 11.75
[BlockManager] shared_cache=True, debug=False
request_time=MetricData(p50=0.20695478677110984, p99=1.417862817119881, max=1.8704740226428964)
prompt_time=MetricData(p50=0.01978855438993654, p99=0.05878943832598336, max=0.09025642535869892)
decode_time=MetricData(p50=0.026783697752379515, p99=0.07915213596980694, max=0.10789018187944072)
decode_max_time=MetricData(p50=0.061347776778529206, p99=0.19747122386541127, max=0.23854410446029958)
prompt_idle_time=MetricData(p50=0.38066459841402844, p99=0.8991646842811908, max=0.9207213061720936)
decode_idle_time=MetricData(p50=0.5268520976501763, p99=1.0, max=1.0)
Average prompt latency: 0.019275720119916327
Max prompt latency: 0.04188317635334746
Min prompt latency: 0.002325461785325611
Average generation latency: 0.025760457995280844
Max generation latency: 0.0680667773312149
Min generation latency: 0.007452825504620606
total time: 1.1071332424240157
Thoughput: 90.3233650369442 r/s, 4443.909559817655 token/s
Total preemptions: 0

Cache Statistics:
Cache Hits: 63
Cache Misses: 37
Cache Hit Rate: 63.00%
Total Allocations: 100
Cache Evictions: 0
Prompt SLO Good Throughput: 427 r, 385.6807687077517 r/s
Max-Decode SLO Good Throughput: 407 r, 367.6160957003629 r/s
Prompt and Max-Decode SLO Good Throughput: 407 r, 367.6160957003629 r/s
Decode SLO Good Throughput: 427 r, 385.6807687077517 r/s
Prompt and Decode SLO Good Throughput: 427 r, 385.6807687077517 r/s
==========

Model = mistral_7b | Dataset = longbench

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/longbench.json",prompt average 60.85, gen average 17.65
[BlockManager] shared_cache=True, debug=False
request_time=MetricData(p50=0.21665084505882076, p99=1.468693443929704, max=1.8704740226428964)
prompt_time=MetricData(p50=0.020946320517962247, p99=0.05438832222159302, max=0.09025642535869892)
decode_time=MetricData(p50=0.02820903976402193, p99=0.08534576348312119, max=0.10789018187944072)
decode_max_time=MetricData(p50=0.06588520257204777, p99=0.2030886076140611, max=0.23854410446029958)
prompt_idle_time=MetricData(p50=0.38420082857536964, p99=0.9004186656011166, max=0.9207213061720936)
decode_idle_time=MetricData(p50=0.5381989365008976, p99=1.0, max=1.0)
Average prompt latency: 0.022406895941536362
Max prompt latency: 0.045251481203935004
Min prompt latency: 0.0022279765037060972
Average generation latency: 0.0333651871122818
Max generation latency: 0.09888427926788143
Min generation latency: 0.011845819566933225
total time: 1.7826531933028122
Thoughput: 56.096160697821944 r/s, 3413.451378462465 token/s
Total preemptions: 0

Cache Statistics:
Cache Hits: 50
Cache Misses: 50
Cache Hit Rate: 50.00%
Total Allocations: 100
Cache Evictions: 0
Prompt SLO Good Throughput: 527 r, 295.6267668775216 r/s
Max-Decode SLO Good Throughput: 500 r, 280.4808034891097 r/s
Prompt and Max-Decode SLO Good Throughput: 500 r, 280.4808034891097 r/s
Decode SLO Good Throughput: 527 r, 295.6267668775216 r/s
Prompt and Decode SLO Good Throughput: 527 r, 295.6267668775216 r/s
==========

Model = mistral_7b | Dataset = needle

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/needle.json",prompt average 1020.0, gen average 5.0
[BlockManager] shared_cache=True, debug=False
request_time=MetricData(p50=0.18376071332291333, p99=1.4243667800830595, max=1.8704740226428964)
prompt_time=MetricData(p50=0.020388372579601008, p99=0.07392168303501566, max=0.09025642535869892)
decode_time=MetricData(p50=0.02774931547814387, p99=0.07915213596980694, max=0.10789018187944072)
decode_max_time=MetricData(p50=0.055918549111306226, p99=0.19747122386541127, max=0.23854410446029958)
prompt_idle_time=MetricData(p50=0.3732519363486636, p99=0.8991646842811908, max=0.9207213061720936)
decode_idle_time=MetricData(p50=0.550697207807983, p99=1.0, max=1.0)
Average prompt latency: 0.023772220547122008
Max prompt latency: 0.08424018187775134
Min prompt latency: 0.006115449766679326
Average generation latency: 0.021949334044279064
Max generation latency: 0.04795894447262267
Min generation latency: 0.0012738493512646082
total time: 1.017196304233254
Thoughput: 98.30944094451698 r/s, 100275.62976340731 token/s
Total preemptions: 0

Cache Statistics:
Cache Hits: 97
Cache Misses: 3
Cache Hit Rate: 97.00%
Total Allocations: 100
Cache Evictions: 0
Prompt SLO Good Throughput: 627 r, 616.4001947221215 r/s
Max-Decode SLO Good Throughput: 600 r, 589.8566456671018 r/s
Prompt and Max-Decode SLO Good Throughput: 600 r, 589.8566456671018 r/s
Decode SLO Good Throughput: 627 r, 616.4001947221215 r/s
Prompt and Decode SLO Good Throughput: 627 r, 616.4001947221215 r/s
==========

Model = mistral_7b | Dataset = sharegpt

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/sharegpt.json",prompt average 99.8, gen average 433.2
[BlockManager] shared_cache=True, debug=False
==========

Model = mistral_7b | Dataset = bookcorpus

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/bookcorpus.json",prompt average 5.85, gen average 5.4
[BlockManager] shared_cache=True, debug=False
request_time=MetricData(p50=0.17965504277549701, p99=1.3745090789374772, max=1.8704740226428964)
prompt_time=MetricData(p50=0.020726242401738093, p99=0.07230565612792972, max=0.09025642535869892)
decode_time=MetricData(p50=0.028836449623382488, p99=0.08534576348312119, max=0.10789018187944072)
decode_max_time=MetricData(p50=0.05711694735634454, p99=0.1916028529568658, max=0.23854410446029958)
prompt_idle_time=MetricData(p50=0.3732519363486636, p99=0.9004186656011166, max=0.9207213061720936)
decode_idle_time=MetricData(p50=0.5760563832237182, p99=1.0, max=1.0)
Average prompt latency: 0.01973639704985472
Max prompt latency: 0.037474563233418356
Min prompt latency: 0.002247413565460765
Average generation latency: 0.03969758761715843
Max generation latency: 0.09878457749159271
Min generation latency: 0.008052348833474934
total time: 1.1695043922686537
Thoughput: 85.50630562918691 r/s, 500.21188793074344 token/s
Total preemptions: 0

Cache Statistics:
Cache Hits: 50
Cache Misses: 50
Cache Hit Rate: 50.00%
Total Allocations: 100
Cache Evictions: 0
Prompt SLO Good Throughput: 727 r, 621.6308419241889 r/s
Max-Decode SLO Good Throughput: 700 r, 598.5441394043083 r/s
Prompt and Max-Decode SLO Good Throughput: 700 r, 598.5441394043083 r/s
Decode SLO Good Throughput: 727 r, 621.6308419241889 r/s
Prompt and Decode SLO Good Throughput: 727 r, 621.6308419241889 r/s
==========

Model = mistral_7b | Dataset = wikipedia

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/wikipedia.json",prompt average 3.9, gen average 87.3
[BlockManager] shared_cache=True, debug=False
==========

Model = mistral_7b | Dataset = opencompass

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/opencompass.json",prompt average 49.2, gen average 11.75
[BlockManager] shared_cache=True, debug=False
request_time=MetricData(p50=0.19344060784417869, p99=1.353514523080233, max=1.8704740226428964)
prompt_time=MetricData(p50=0.020411815089999667, p99=0.06772108581865764, max=0.09025642535869892)
decode_time=MetricData(p50=0.02686926812764314, p99=0.07680423895291504, max=0.10789018187944072)
decode_max_time=MetricData(p50=0.061321651323607784, p99=0.2004839732492836, max=0.23854410446029958)
prompt_idle_time=MetricData(p50=0.3823635672328846, p99=0.9007376711022922, max=0.9207213061720936)
decode_idle_time=MetricData(p50=0.5472751041794696, p99=1.0, max=1.0)
Average prompt latency: 0.017929359740937942
Max prompt latency: 0.03895519443758931
Min prompt latency: 0.0020878774156589675
Average generation latency: 0.023855910933598446
Max generation latency: 0.06296451528930529
Min generation latency: 0.005680586272971701
total time: 1.0807952359594222
Thoughput: 92.52446409169261 r/s, 4552.2036333112765 token/s
Total preemptions: 0

Cache Statistics:
Cache Hits: 61
Cache Misses: 39
Cache Hit Rate: 61.00%
Total Allocations: 100
Cache Evictions: 0
Prompt SLO Good Throughput: 854 r, 790.1589233430549 r/s
Max-Decode SLO Good Throughput: 819 r, 757.7753609109625 r/s
Prompt and Max-Decode SLO Good Throughput: 819 r, 757.7753609109625 r/s
Decode SLO Good Throughput: 854 r, 790.1589233430549 r/s
Prompt and Decode SLO Good Throughput: 854 r, 790.1589233430549 r/s
==========

Model = internlm2_7b | Dataset = longbench

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/longbench.json",prompt average 60.85, gen average 17.65
[BlockManager] shared_cache=True, debug=False
request_time=MetricData(p50=0.20018073826722332, p99=1.3810130419006557, max=1.8704740226428964)
prompt_time=MetricData(p50=0.020777118634985353, p99=0.06481949634432604, max=0.09025642535869892)
decode_time=MetricData(p50=0.02764862108147451, p99=0.0839108632071714, max=0.10789018187944072)
decode_max_time=MetricData(p50=0.061347776778529206, p99=0.2030657578850162, max=0.23854410446029958)
prompt_idle_time=MetricData(p50=0.38420082857536964, p99=0.9014860088628114, max=0.9207213061720936)
decode_idle_time=MetricData(p50=0.5565855915888486, p99=1.0, max=1.0)
Average prompt latency: 0.022406895941536362
Max prompt latency: 0.045251481203935004
Min prompt latency: 0.0022279765037060972
Average generation latency: 0.0333651871122818
Max generation latency: 0.09888427926788143
Min generation latency: 0.011845819566933225
total time: 1.7826531933028122
Thoughput: 56.096160697821944 r/s, 3413.451378462465 token/s
Total preemptions: 0

Cache Statistics:
Cache Hits: 50
Cache Misses: 50
Cache Hit Rate: 50.00%
Total Allocations: 100
Cache Evictions: 0
Prompt SLO Good Throughput: 954 r, 535.1573730572213 r/s
Max-Decode SLO Good Throughput: 912 r, 511.5969855641361 r/s
Prompt and Max-Decode SLO Good Throughput: 912 r, 511.5969855641361 r/s
Decode SLO Good Throughput: 954 r, 535.1573730572213 r/s
Prompt and Decode SLO Good Throughput: 954 r, 535.1573730572213 r/s
==========

Model = internlm2_7b | Dataset = needle

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/needle.json",prompt average 1020.0, gen average 5.0
[BlockManager] shared_cache=True, debug=False
request_time=MetricData(p50=0.18376071332291333, p99=1.3677549635526383, max=1.8704740226428964)
prompt_time=MetricData(p50=0.02051434113780018, p99=0.07423650329966747, max=0.09025642535869892)
decode_time=MetricData(p50=0.027243743696601076, p99=0.07680423895291504, max=0.10789018187944072)
decode_max_time=MetricData(p50=0.05538957799606814, p99=0.2004839732492836, max=0.23854410446029958)
prompt_idle_time=MetricData(p50=0.3783156962721878, p99=0.9007376711022922, max=0.9207213061720936)
decode_idle_time=MetricData(p50=0.5582416941072071, p99=1.0, max=1.0)
Average prompt latency: 0.023772220547122008
Max prompt latency: 0.08424018187775134
Min prompt latency: 0.006115449766679326
Average generation latency: 0.021949334044279064
Max generation latency: 0.04795894447262267
Min generation latency: 0.0012738493512646082
total time: 1.017196304233254
Thoughput: 98.30944094451698 r/s, 100275.62976340731 token/s
Total preemptions: 0

Cache Statistics:
Cache Hits: 97
Cache Misses: 3
Cache Hit Rate: 97.00%
Total Allocations: 100
Cache Evictions: 0
Prompt SLO Good Throughput: 1054 r, 1036.181507555209 r/s
Max-Decode SLO Good Throughput: 1012 r, 994.8915423585119 r/s
Prompt and Max-Decode SLO Good Throughput: 1012 r, 994.8915423585119 r/s
Decode SLO Good Throughput: 1054 r, 1036.181507555209 r/s
Prompt and Decode SLO Good Throughput: 1054 r, 1036.181507555209 r/s
==========

Model = internlm2_7b | Dataset = sharegpt

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/sharegpt.json",prompt average 99.8, gen average 433.2
[BlockManager] shared_cache=True, debug=False
==========

Model = internlm2_7b | Dataset = bookcorpus

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/bookcorpus.json",prompt average 5.85, gen average 5.4
[BlockManager] shared_cache=True, debug=False
request_time=MetricData(p50=0.18103464324377536, p99=1.353514523080233, max=1.8704740226428964)
prompt_time=MetricData(p50=0.020726242401738093, p99=0.07359475429864648, max=0.09025642535869892)
decode_time=MetricData(p50=0.027940686167892352, p99=0.0839108632071714, max=0.10789018187944072)
decode_max_time=MetricData(p50=0.05569518970849868, p99=0.19560527543937004, max=0.23854410446029958)
prompt_idle_time=MetricData(p50=0.3783156962721878, p99=0.9014860088628114, max=0.9207213061720936)
decode_idle_time=MetricData(p50=0.5740717024684093, p99=1.0, max=1.0)
Average prompt latency: 0.01973639704985472
Max prompt latency: 0.037474563233418356
Min prompt latency: 0.002247413565460765
Average generation latency: 0.03969758761715843
Max generation latency: 0.09878457749159271
Min generation latency: 0.008052348833474934
total time: 1.1695043922686537
Thoughput: 85.50630562918691 r/s, 500.21188793074344 token/s
Total preemptions: 0

Cache Statistics:
Cache Hits: 50
Cache Misses: 50
Cache Hit Rate: 50.00%
Total Allocations: 100
Cache Evictions: 0
Prompt SLO Good Throughput: 1154 r, 986.742766960817 r/s
Max-Decode SLO Good Throughput: 1112 r, 950.8301185965585 r/s
Prompt and Max-Decode SLO Good Throughput: 1112 r, 950.8301185965585 r/s
Decode SLO Good Throughput: 1154 r, 986.742766960817 r/s
Prompt and Decode SLO Good Throughput: 1154 r, 986.742766960817 r/s
==========

Model = internlm2_7b | Dataset = wikipedia

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/wikipedia.json",prompt average 3.9, gen average 87.3
[BlockManager] shared_cache=True, debug=False
==========

Model = internlm2_7b | Dataset = opencompass

[get_lens_from_file()] Read 100 reqs from dataset "./dataset/opencompass.json",prompt average 49.2, gen average 11.75
[BlockManager] shared_cache=True, debug=False
request_time=MetricData(p50=0.19088053463888066, p99=1.3513123720848648, max=1.8704740226428964)
prompt_time=MetricData(p50=0.02051434113780018, p99=0.07302565612792969, max=0.09025642535869892)
decode_time=MetricData(p50=0.02695483850290677, p99=0.07445634193602312, max=0.10789018187944072)
decode_max_time=MetricData(p50=0.06113145077165072, p99=0.20185510598154227, max=0.23854410446029958)
prompt_idle_time=MetricData(p50=0.38254917875208727, p99=0.9010840056377272, max=0.9207213061720936)
decode_idle_time=MetricData(p50=0.5556975284741015, p99=1.0, max=1.0)
Average prompt latency: 0.017929359740937942
Max prompt latency: 0.03895519443758931
Min prompt latency: 0.0020878774156589675
Average generation latency: 0.023855910933598446
Max generation latency: 0.06296451528930529
Min generation latency: 0.005680586272971701
total time: 1.0807952359594222
Thoughput: 92.52446409169261 r/s, 4552.2036333112765 token/s
Total preemptions: 0

Cache Statistics:
Cache Hits: 61
Cache Misses: 39
Cache Hit Rate: 61.00%
Total Allocations: 100
Cache Evictions: 0
Prompt SLO Good Throughput: 1281 r, 1185.2383850145823 r/s
Max-Decode SLO Good Throughput: 1231 r, 1138.976152968736 r/s
Prompt and Max-Decode SLO Good Throughput: 1231 r, 1138.976152968736 r/s
Decode SLO Good Throughput: 1281 r, 1185.2383850145823 r/s
Prompt and Decode SLO Good Throughput: 1281 r, 1185.2383850145823 r/s
